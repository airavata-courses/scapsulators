{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Access MERRA-2 Data using OPeNDAP with Python3 and Calculate Daily/Weekly/Monthly Statistics from Hourly Data \n",
    "\n",
    "* This instruction is based on Python3 and demonstrates how to remotely access the Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA-2) hourly files via OPeNDAP and analyze data such as resample hourly files into daily, weekly, and monthly files and calculate their corresponding statistics, e.g., mean, sum, maximum, and minimum. \n",
    "\n",
    "**Contact**: \n",
    "* gsfc-dl-help-disc@mail.nasa.gov\n",
    "* Last update: Sep. 20, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python3 example code demonstrates how to remotely access a dataset archived in GES DISC using the Open-source Project for a Network Data Access Protocol (OPeNDAP) web service. We use the Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA-2) aerosol diagnostics collection M2T1NXAER.5.12.4 in this example. This collection is 1-hourly time-averaged single-level global aerosol assimilation and archived in daily files with 24 hourly time slices in each file (481 MB per file/day and 14.6 GB per month). For demonstration, we only read 12-day data (i.e., the first 12 days in January 2020)remotely through OPeNDAP URL. We also demonstrate how to calculate the daily/weekly/monthly statistics from hourly data and visualize the evolution of Australian bushfire in January 2020. Figures 1 and 2 are the example images generated by the Python code below, in which the total aerosol extinction (AOT) is plotted as an indicator of the aerosol loading in atmosphere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites** \n",
    "\n",
    "- This example code is written in Python3 (v3.9.2) Jupyter Notebook and requires these libraries: xarray (0.17.0), matplotlib.pyplot (3.4.1), cartopy.crs (0.18.0), calendar, time, platform (make sure all packages are up to date). In particular, here is the instruction on how to install [xarray](http://xarray.pydata.org/en/stable/getting-started-guide/installing.html) and [cartopy](https://scitools.org.uk/cartopy/docs/v0.15/installing.html). \n",
    "- You can execute this example code in your Jupyter Notebook. This code has been tested with Jupyter Notebook v6.2.0 and v6.3.0 in Mac OS, Jupyter Notebook v6.1.4 in Windows OS. Or you can just run it in your Python 3 enviroment. This code has been tested in Python 3 in Mac, window and Linux OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caveats**:\n",
    "\n",
    "- Reading multiple hourly data files is a resource demanding task due to large data volume. It may take about 5 minutes to open one-month of the sample data (or longer if the data archive system is  currently heavily loaded). Be patient!\n",
    "- Visualizing the figures may also take time (~4 minute)\n",
    "- You may want to test first if your xarray package can read the data in your local disk before reading the data remotely as demonstrated in this case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**:\n",
    "- [Time-series in xarray](http://xarray.pydata.org/en/stable/time-series.html) \n",
    "- [Statistical Operations, Resampling and Climatologies Using Xarray](https://nci-data-training.readthedocs.io/en/latest/_notebook/climate/1_07_Xarray_statistical_resample_roll_climatology_CMIP6.html)\n",
    "- [Xarray plotting and visualization](https://xarray-contrib.github.io/xarray-tutorial/scipy-tutorial/04_plotting_and_visualization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procedure**:\n",
    "1. Register Earthdata account and set up the credential environment\n",
    "\n",
    "2. Execute the Python code below in your Jupyter Notebook step-by-step\n",
    "- 2.1 Import the required Python modules or libraries. If any of the following import commands fail, check the local Python environment and install any missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Import Python modules\n",
    "# ----------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xarray as xr, matplotlib.pyplot as plt, cartopy.crs as ccrs, time, platform\n",
    "from calendar import monthrange\n",
    "\n",
    "print(\"platform.python_version() \", platform.python_version())\n",
    "print(xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "# ADD 'ascii' between url and request-params\n",
    "# SAMPLE URL 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXOCN.5.12.4/2018/09/MERRA2_400.tavg1_2d_ocn_Nx.20180906.nc4.ascii?LWGNTICE[0:1:23][0:1:360][0:1:575],lat[0:1:360],lon[0:1:575],time[0:1:23]'\n",
    "\n",
    "REQ_FORMAT = 'ascii'\n",
    "collection_shortname = 'M2T1NXOCN'\n",
    "collection_longname  = 'tavg1_2d_ocn_Nx'\n",
    "collection_number = 'MERRA2_400'  \n",
    "MERRA2_version = '5.12.4'\n",
    "year = '2018'\n",
    "month = '9'.zfill(2)\n",
    "day = '1'.zfill(2)\n",
    "FEATURE = 'LWGNTICE'\n",
    "TIME_WINDOWS = '[0:6:23]' # 24 hours divided into 6 hour chunks\n",
    "LAT_WINDOWS = '[0:1:360]' # 360 deg latitudes\n",
    "LONG_WINDOWS = '[0:1:575]' # 575 deg longitudes\n",
    "PARAMS_TO_RECEIVE = [\n",
    "    FEATURE+TIME_WINDOWS+LAT_WINDOWS+LONG_WINDOWS, \n",
    "    'lat'+LAT_WINDOWS, \n",
    "    'lon'+LONG_WINDOWS, \n",
    "    'time'+TIME_WINDOWS\n",
    "    ]\n",
    "tgt_filename = 'temp.txt'\n",
    "\n",
    "\n",
    "folder_url = f'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/{collection_shortname}.{MERRA2_version}/{year}/{month}/'\n",
    "file_url = f'{collection_number}.{collection_longname}.{\"\".join([year,month,day])}.nc4.{REQ_FORMAT}'\n",
    "url = folder_url + file_url\n",
    "url_params = f'{\",\".join(PARAMS_TO_RECEIVE)}'\n",
    "\n",
    "\n",
    "response = get(url=url, params=url_params, auth=(''))\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "    with open(tgt_filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "except Exception as e:\n",
    "    print('Something went wrong with API GET request.')\n",
    "    print(e)\n",
    "finally:\n",
    "    print('Data Ingestion complete...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN CODE FOR DATA PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Data Ingestion complete...\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "import re, reverse_geocoder as rg, pycountry as pyctry\n",
    "\n",
    "response = get(\n",
    "    url='https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXOCN.5.12.4/2018/09/MERRA2_400.tavg1_2d_ocn_Nx.20180906.nc4.ascii', \n",
    "    params='TSKINICE[0:4:23][0:5:360][0:5:575],lat[0:5:360],lon[0:5:575],time[0:4:23]'\n",
    "    )\n",
    "print(response.status_code)\n",
    "tgt_filename = r'.\\temp.txt'\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "    with open(tgt_filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "except Exception as e:\n",
    "    print('Something went wrong with API GET request.')\n",
    "    print(e)\n",
    "finally:\n",
    "    print('Data Ingestion complete...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_response_to_json(filename='temp.txt', verbose=False):\n",
    "    data = None\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    json_data = {}\n",
    "    for line in data[1:]:\n",
    "        parts = line.split(',')\n",
    "        key, values = parts[0], [value.strip() for value in parts[1:]]\n",
    "        json_data[key] = values\n",
    "    if verbose:  print(json_data.keys())\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_country_measurements(locations_vs_measurements, verbose=False):\n",
    "    if verbose:  print('Creating a {country:(sum,count)} dictionary...')\n",
    "    country_measurements = dict()\n",
    "    all_locations = list(locations_vs_measurements.keys())\n",
    "    all_locations_mapped = rg.search(all_locations)\n",
    "    for loc, country in zip(all_locations, all_locations_mapped):\n",
    "        if verbose:  print(loc, country['cc'], locations_vs_measurements[loc])\n",
    "        if country['cc'] not in country_measurements:\n",
    "            country_measurements[country['cc']] = [0,0]\n",
    "        country_measurements[country['cc']][0] += locations_vs_measurements[loc][0]\n",
    "        country_measurements[country['cc']][1] += 1\n",
    "\n",
    "    if verbose:  print('Normalizing the measurements taken for each country...')\n",
    "    total = 0\n",
    "    for country, (total,counts) in country_measurements.items():\n",
    "        country_measurements[country] = country_measurements[country][0]/country_measurements[country][1]\n",
    "        total += country_measurements[country]\n",
    "\n",
    "    for country, val in country_measurements.items():\n",
    "        country_measurements[country] = (val/total)*100\n",
    "\n",
    "    return country_measurements\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def parse_json_dataset(json_data, verbose=False):\n",
    "    json_res = dict()\n",
    "    for t, time_val in enumerate(json_data['time']):\n",
    "        if verbose:  print(f'Getting measurements across all (latitudes,longitudes) within timestep {time_val}..')\n",
    "        latitude_rows = []\n",
    "        for k in json_data.keys():\n",
    "            if re.findall(f'\\[{t}\\]\\[.*\\]', k):\n",
    "                if verbose:  print(k)\n",
    "                latitude_rows.append(k)\n",
    "\n",
    "        locations_vs_measurements = dict()\n",
    "        for i,latitude_row in enumerate(latitude_rows):\n",
    "            latitude = int(float(json_data['lat'][i]))\n",
    "            longitudinal_measurements = json_data[latitude_row]\n",
    "            for j, measurement in enumerate(longitudinal_measurements):\n",
    "                longitude = int(float(json_data['lon'][j]))\n",
    "                location = (latitude,longitude)\n",
    "                if location not in locations_vs_measurements:\n",
    "                    locations_vs_measurements[location] = []\n",
    "                locations_vs_measurements[location].append(100 if measurement=='1e+15' else float(measurement))\n",
    "        json_res[t] = get_country_measurements(locations_vs_measurements, verbose=False)\n",
    "        \n",
    "    return json_res\n",
    "\n",
    "\n",
    "json_data = convert_response_to_json(filename=tgt_filename)\n",
    "time_vs_country_measurements = parse_json_dataset(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_file = 'temp.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(time_vs_country_measurements, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2 Remotely access the hourly MERRA-2 files through OPeNDAP URL (Please refer to \"How to obtain the URL of OPeNDAP served dataset\"). In this case below, we read the first 12 days in January 2020. Note that the collection number was changed to a new number if this collection was reprocessed, please refer to [\"Records of MERRA-2 Data Reprocessing and Service Changes\"](https://disc.gsfc.nasa.gov/information/documents?title=Records%20of%20MERRA-2%20Data%20Reprocessing%20and%20Service%20Changes). The line of \"%%time\" at the beginning of each cell is used for estimating the running time for that cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ---------------------------------\n",
    "# Read data\n",
    "# ---------------------------------\n",
    "# MERRA-2 collection (hourly)\n",
    "collection_shortname = 'M2T1NXAER'\n",
    "collection_longname  = 'tavg1_2d_aer_Nx'\n",
    "collection_number = 'MERRA2_400'  \n",
    "MERRA2_version = '5.12.4'\n",
    "year = 2020\n",
    "    \n",
    "# Open dataset\n",
    "# Read selected days in the same month and year\n",
    "month = 1  # January\n",
    "day_beg = 1\n",
    "day_end = 12\n",
    "    \n",
    "# Note that collection_number is MERRA2_401 in a few cases, refer to \"Records of MERRA-2 Data Reprocessing and Service Changes\"\n",
    "if year == 2020 and month == 9:\n",
    "    collection_number = 'MERRA2_401'\n",
    "            \n",
    "# OPeNDAP URL \n",
    "url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/{}.{}/{}/{:0>2d}'.format(collection_shortname, MERRA2_version, year, month)\n",
    "files_month = ['{}/{}.{}.{}{:0>2d}{:0>2d}.nc4'.format(url,collection_number, collection_longname, year, month, days) for days in range(day_beg,day_end+1)]\n",
    "# Get the number of files\n",
    "len_files_month=len(files_month)\n",
    "\n",
    "# Print\n",
    "print(\"{} files to be opened:\".format(len_files_month))\n",
    "print(\"files_month\", files_month)\n",
    "\n",
    "print(\" \")  \n",
    "print(\"Opening...(It may take ~ 5 minutes to open 1-month data)\")\n",
    "print(\" \")\n",
    "    \n",
    "# Read dataset URLs\n",
    "ds = xr.open_mfdataset(files_month)\n",
    "   \n",
    "# View metadata (function like ncdump -c)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.3 Select your interested variables and extract it from the whole dataset. In this case, we chose \"TOTEXTTAU\" (Total Aerosol Extinction) at 550nm as an indicator of the aerosol loading in atmosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Select your interested variable (e.g., TOTEXTTAU)\n",
    "# ---------------------------------------------------\n",
    "sel_var_shortname = \"TOTEXTTAU\"\n",
    "sel_var_value = ds[sel_var_shortname]\n",
    "sel_var_longname = sel_var_value.attrs['long_name']\n",
    "sel_var_unit = '('+sel_var_value.attrs['units']+')' \n",
    "print(\"The selected variable is {}: {}{}\".format(sel_var_shortname, sel_var_longname,sel_var_unit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4 Calculate the daily/weekly/monthly statistics from hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Resample hourly files into daily, weekly, and monthly files and calculate their corresponding statistics,\n",
    "# e.g., mean, sum, maximum, and minimum.  \n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Functions used to calculate various statistics\n",
    "# ===================================\n",
    "# Purpose           Function\n",
    "# \n",
    "# mean of dim       mean\n",
    "# sum of dim        sum\n",
    "# max of dim        max\n",
    "# min of dim        min\n",
    "# ==================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the daily mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily mean (i.e., the averaged value over a day at each grid)\n",
    "sel_var_daily_mean = sel_var_value.resample(time=\"1D\").mean(dim='time', skipna=True)\n",
    "sel_var_daily_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the weekly maximum. Note that it is calendar week, starting from Monday to Sunday of each week. Only available data are counted towards each week. For example, for the first week of 2020, it only counts Jan. 1, 2020 (Wed.), which is the first day read in, to Jan. 5, 2020 (Sun.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Weekly maximum (i.e., the maximum of each week at each grid)\n",
    "sel_var_weekly_max = sel_var_value.resample(time=\"1w\").max(dim='time', skipna=True)\n",
    "sel_var_weekly_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get monthly total (it doesn't have any physical meaning for this case, AOT. The sum is useful for mass related variable, such as precipitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# e.g., monthly total (i.e., the total amount over a month at each grid)\n",
    "# sel_var_monthly_total = sel_var_value.resample(time='m').sum(dim='time', skipna=True)\n",
    "# sel_var_monthly_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.5 Visualize the evolution of Australian bushfire with maps and time series\n",
    "\n",
    "1) Plot the spatial map (e.g., the first two weeks). See Figure 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ------------------------------------------------------------------\n",
    "# Visualizing the evolution of Australian bushfire \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# 1) Plot the spatial facet map (e.g., the first two weeks)\n",
    "print(\"Plotting...(It may take ~ 4 minutes)\")\n",
    "\n",
    "pmap = sel_var_weekly_max.isel(time=[0,1]).plot(transform=ccrs.PlateCarree(),  # the data's projection\n",
    "             col='time', col_wrap=2, robust=True, # multiplot settings\n",
    "             cmap=plt.cm.Spectral_r, \n",
    "             cbar_kwargs={\n",
    "            \"orientation\": \"horizontal\",\n",
    "            \"shrink\": 0.9,\n",
    "            \"aspect\": 40,\n",
    "            \"pad\": 0.1,\n",
    "                         },\n",
    "            subplot_kws={'projection': ccrs.PlateCarree(central_longitude=180)})  # the plot's projection\n",
    "            # shift the original central longitude from 0 to 180 \n",
    "\n",
    "\n",
    "# We have to set the map's options on all axes\n",
    "\n",
    "for ax in pmap.axes.flat:\n",
    "    ax.coastlines(resolution=\"110m\",linewidth=0.5)\n",
    "    \n",
    "# Plot main title\n",
    "main_title = \"{}{} weekly_max\".format(sel_var_longname, sel_var_unit) \n",
    "plt.suptitle(main_title, fontweight='bold')\n",
    "\n",
    "# Save the plot \n",
    "file_dir = '.'\n",
    "figFile_plot = \"{}/map.{}.selected_time.{}.png\".format(file_dir, collection_shortname,sel_var_shortname)\n",
    "plt.savefig(figFile_plot, dpi=200)\n",
    "print(\" Pick up your figure \", figFile_plot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Plot the time series of daily mean averaged over the globe (Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2) Plot the time series of daily mean averaged over the globe \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,5.5))\n",
    "sel_var_daily_mean_region = sel_var_daily_mean.groupby('time').mean(dim=['lat','lon'],skipna=True)\n",
    "\n",
    "# Convert to dataframe \n",
    "plotdata = sel_var_daily_mean_region.to_dataframe()\n",
    "\n",
    "# List the statistics\n",
    "stat = plotdata.describe()\n",
    "print(\"stat:\")\n",
    "print(stat)\n",
    "\n",
    "# Plot time series\n",
    "ax.plot(plotdata,label='daily_mean', marker=\"o\", linewidth=2)\n",
    "ax.legend(shadow=True, fancybox=True)\n",
    "\n",
    "# Plot main title and xy labels\n",
    "main_title = \"{}{} daily_mean averaged over the globe\".format(sel_var_longname, sel_var_unit) \n",
    "plt.suptitle(main_title, fontweight='bold')\n",
    "ax.set_ylabel(sel_var_shortname+sel_var_unit)\n",
    "\n",
    "# Save the plot\n",
    "figFile_plot = \"{}/timeseries.{}.selected_time.{}.png\".format(file_dir, collection_shortname,sel_var_shortname)\n",
    "plt.savefig(figFile_plot, dpi=200)\n",
    "print(\" Pick up your figure \", figFile_plot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
